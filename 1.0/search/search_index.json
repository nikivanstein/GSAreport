{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Global Sensitivity Analysis Reporting#G lobal Sensitivity Analysis Reporting GSAreport is an application to easily generate reports that describe the global sensitivities of your input parameters as best as possible. You can use the reporting application to inspect which features are important for a given real world function / simulator or model. Using the dockerized application you can generate a report with just one line of code and no additional dependencies (except for Docker of course). Global Sensitivity Analysis is one of the tools to better understand your machine learning models or get an understanding in real-world processes. What is Sensitivity Analysis? According to Wikipedia, sensitivity analysis is \"the study of how the uncertainty in the output of a mathematical model or system (numerical or otherwise) can be apportioned to different sources of uncertainty in its inputs.\" The sensitivity of each input is often represented by a numeric value, called the sensitivity index. Sensitivity indices come in several forms: First-order indices: measures the contribution to the output variance by a single model input alone. Second-order indices: measures the contribution to the output variance caused by the interaction of two model inputs. Total-order index: measures the contribution to the output variance caused by a model input, including both its first-order effects (the input varying alone) and all higher-order interactions. Sensitivity Analysis is a great way of getting a better understanding of how machine learning models work (Explainable AI), what parameters are of importance in real-world applications and processes and what interactions parameters have with other parameters. GSAreport makes it easy to run a wide set of SA techniques and generates a nice and visually attractive report to inspect the results of these techniques. By using Docker no additional software needs to be installed and no coding experience is required.","title":"GSA report"},{"location":"#global-sensitivity-analysis-reportingg-lobal-sensitivity-analysis-reporting","text":"GSAreport is an application to easily generate reports that describe the global sensitivities of your input parameters as best as possible. You can use the reporting application to inspect which features are important for a given real world function / simulator or model. Using the dockerized application you can generate a report with just one line of code and no additional dependencies (except for Docker of course). Global Sensitivity Analysis is one of the tools to better understand your machine learning models or get an understanding in real-world processes.","title":"Global Sensitivity Analysis Reporting#G lobal Sensitivity Analysis Reporting"},{"location":"#what-is-sensitivity-analysis","text":"According to Wikipedia, sensitivity analysis is \"the study of how the uncertainty in the output of a mathematical model or system (numerical or otherwise) can be apportioned to different sources of uncertainty in its inputs.\" The sensitivity of each input is often represented by a numeric value, called the sensitivity index. Sensitivity indices come in several forms: First-order indices: measures the contribution to the output variance by a single model input alone. Second-order indices: measures the contribution to the output variance caused by the interaction of two model inputs. Total-order index: measures the contribution to the output variance caused by a model input, including both its first-order effects (the input varying alone) and all higher-order interactions. Sensitivity Analysis is a great way of getting a better understanding of how machine learning models work (Explainable AI), what parameters are of importance in real-world applications and processes and what interactions parameters have with other parameters. GSAreport makes it easy to run a wide set of SA techniques and generates a nice and visually attractive report to inspect the results of these techniques. By using Docker no additional software needs to be installed and no coding experience is required.","title":"What is Sensitivity Analysis?"},{"location":"contributing/","text":"Contributing First off, thanks for taking the time to contribute! File an issue to notify the maintainers about what you're working on. Fork the repo, develop and test your code changes, add docs. Make sure that your commit messages clearly describe the changes. Send a pull request. File an Issue Use the issue tracker to start the discussion. It is possible that someone else is already working on your idea, your approach is not quite right, or that the functionality exists already. The ticket you file in the issue tracker will be used to hash that all out. Style Guides Write in UTF-8 in Python 3 User modular architecture to group similar functions, classes, etc. Always use 4 spaces for indentation (don't use tabs) Try to limit line length to 80 characters Class names should always be capitalized Function names should always be lowercase Look at the existing style and adhere accordingly Fork the Repository Be sure to add the relevant tests before making the pull request. Docs will be updated automatically when we merge to master , but you should also build the docs yourself and make sure they're readable. Make the Pull Request Once you have made all your changes, tests, and updated the documentation, make a pull request to move everything back into the main branch of the repository . Be sure to reference the original issue in the pull request. Expect some back-and-forth with regards to style and compliance of these rules. Building binaries (for developers) If you want to build the executables yourself you can use the following commands. We use pyinstaller to package the executables. Make sure you have pyinstaller installed using pip install pyinstaller . On your operating system, build the exe once you have the python source code up and running: pyinstaller --distpath dist/darwin/ GSAreport.spec We provide binaries for Linux and Mac-OS in the releases section.","title":"How to contribute"},{"location":"contributing/#contributing","text":"First off, thanks for taking the time to contribute! File an issue to notify the maintainers about what you're working on. Fork the repo, develop and test your code changes, add docs. Make sure that your commit messages clearly describe the changes. Send a pull request.","title":"Contributing"},{"location":"contributing/#file-an-issue","text":"Use the issue tracker to start the discussion. It is possible that someone else is already working on your idea, your approach is not quite right, or that the functionality exists already. The ticket you file in the issue tracker will be used to hash that all out.","title":"File an Issue"},{"location":"contributing/#style-guides","text":"Write in UTF-8 in Python 3 User modular architecture to group similar functions, classes, etc. Always use 4 spaces for indentation (don't use tabs) Try to limit line length to 80 characters Class names should always be capitalized Function names should always be lowercase Look at the existing style and adhere accordingly","title":"Style Guides"},{"location":"contributing/#fork-the-repository","text":"Be sure to add the relevant tests before making the pull request. Docs will be updated automatically when we merge to master , but you should also build the docs yourself and make sure they're readable.","title":"Fork the Repository"},{"location":"contributing/#make-the-pull-request","text":"Once you have made all your changes, tests, and updated the documentation, make a pull request to move everything back into the main branch of the repository . Be sure to reference the original issue in the pull request. Expect some back-and-forth with regards to style and compliance of these rules.","title":"Make the Pull Request"},{"location":"contributing/#building-binaries-for-developers","text":"If you want to build the executables yourself you can use the following commands. We use pyinstaller to package the executables. Make sure you have pyinstaller installed using pip install pyinstaller . On your operating system, build the exe once you have the python source code up and running: pyinstaller --distpath dist/darwin/ GSAreport.spec We provide binaries for Linux and Mac-OS in the releases section.","title":"Building binaries (for developers)"},{"location":"installation/","text":"Using Docker (Recommended) The easiest way to use the GSAreport application is directly using docker. This way you do not need to install any third party software. Install docker (https://docs.docker.com/get-docker/) Run the image emeraldit/gsareport as container with a volume for your data and for the output generated. Example to show help text: docker run -v ` pwd ` /output:/output -v ` pwd ` /data:/data emeraldit/gsareport -h Using executables If you cannot or do not want to install Docker, you can also use the pre-compiled executables from the Releases section. The executables do not contain graph-tool support and will not generate a sobol network plot, all other functionality is included. You can use the executables from the command line with the same console parameters as explained in the Quick start section. Graph tool support Note that when using the precompiled executables the Sobol network plots are not included in the reports. The executables do not have support for the graph-tool package. Using python source You can also use the package by installing the dependencies to your own system. Install graph-tool (https://graph-tool.skewed.de/) Install python 3.7+ Install node (v14+) Clone the repository with git or download the zip Install all python requirements ( pip install -r src/requirements.txt ) Run python src/GSAreport.py -h","title":"Installation"},{"location":"installation/#using-docker-recommended","text":"The easiest way to use the GSAreport application is directly using docker. This way you do not need to install any third party software. Install docker (https://docs.docker.com/get-docker/) Run the image emeraldit/gsareport as container with a volume for your data and for the output generated. Example to show help text: docker run -v ` pwd ` /output:/output -v ` pwd ` /data:/data emeraldit/gsareport -h","title":"Using Docker (Recommended)"},{"location":"installation/#using-executables","text":"If you cannot or do not want to install Docker, you can also use the pre-compiled executables from the Releases section. The executables do not contain graph-tool support and will not generate a sobol network plot, all other functionality is included. You can use the executables from the command line with the same console parameters as explained in the Quick start section. Graph tool support Note that when using the precompiled executables the Sobol network plots are not included in the reports. The executables do not have support for the graph-tool package.","title":"Using executables"},{"location":"installation/#using-python-source","text":"You can also use the package by installing the dependencies to your own system. Install graph-tool (https://graph-tool.skewed.de/) Install python 3.7+ Install node (v14+) Clone the repository with git or download the zip Install all python requirements ( pip install -r src/requirements.txt ) Run python src/GSAreport.py -h","title":"Using python source"},{"location":"references/","text":"This tool uses Savvy [1] and SALib [2]. Blake Hough, ., Chris Fu, ., & Swapil Paliwal, . (2016). savvy: visualize high dimensionality sensitivity analysis data. Updated with full sensitivity analysis from ligpy model. (v2.0). Zenodo. https://doi.org/10.5281/zenodo.53099 Herman, J. and Usher, W. (2017) SALib: An open-source Python library for sensitivity analysis. Journal of Open Source Software, 2(9). doi:10.21105/joss.00097","title":"References"},{"location":"usecases/","text":"Generate a global sensitivity analysis report for a given data set or function with the simple Docker / python or executable command options. To start, you always need to provide the program with a problem definition. This definition can be supplied as json file, see also data/problem.json for an example. The problem definition contains the dimensionality of your problem (number of input variables) num_vars , the names of these variables (X0 to X4 in the example), and the bounds of each variables as a list of tuples (lower bound, upper bound). #Example problem definition in python (you can store this dict using json.dump to a json file) dim = 5 problem = { 'num_vars' : dim , 'names' : [ 'X' + str ( x ) for x in range ( dim )], 'bounds' : [[ - 5.0 , 5.0 ]] * dim } Once you have the problem definition (specify it with -p path/to/problem.json ) you can directly load an existing data set containing input and output files for analysis by passing the path to the directory (with -d <path> ) in which these files are stored. The application searches for the following csv files: x.csv, y.csv #optional, in case you use an existing design of experiments x_sobol.csv, y_sobol.csv x_morris.csv, y_morris.csv x_lhs.csv, y_lhs.csv When you have your own design of experiments you can store these in x and y.csv (space delimited). The Sobol, Morris and LHS (Latin Hypercube Sampling) files can be used when you have samples and results from a specific sampling technique which can be used for different Sensitivity analysis algorithms. The GSA report application can generate the x_ version of these files (the input). Using the input files you can then evaluate the data points and store the target values y in the csv file with the same name convention. If you only provide an x.csv and y.csv file, a machine learning algorithm will be used to interpolate the remaining samples to generate the appropriate design of experiments required for the sensitivity analysis. A python example to read the x_*.csv files and produce the correspondig y_*.csv files using your own objective function is provided in the next section. Common use cases There are three main steps in using the GSA report application, first to generate designs of experiments (the input files), second to evaluate these design of experiments and store them as y_*.csv files (using your own logic / simulators / real world experiments), and last but not least to load the data and perform the sensitivity analysis. To generate the samples for evaluation by your own code / simulator you can run the following docker command: Docker Python Executable docker run --rm \\ -v ` pwd ` /data:/data \\ emeraldit/gsareport -p /data/problem.json -d /data --sample --samplesize 1000 Here we run a docker image called emeraldit/gsareport , which is the GSAreport program packaged with all the required dependencies. The following line that start with -v creates a volume, sharing the folder data in our current working directory with the docker image (in location /data on the image). That way the program can access the data directory to store the design of experiment files ( x_*.csv ). python GSAreport.py -p problem.json -d ` pwd ` /data --sample --samplesize 1000 ./GSAreport -p problem.json -d ` pwd ` /data --sample --samplesize 1000 We give the following parameters to the program, -p to specify where to find the problem.json file (in the shared volume), -d to specify where to find the data, --sample to tell the program to generate the samples and --samplesize 1000 to specify that we want designs of experiments with 1000 samples. After running this step we will have 3 .csv files in our pwd /data folder (X_sobol.csv, X_lhs.csv and X_morris.csv). Using these 3 files we can generate the corresponding output files (outside of this software) using any tool. In this example we run a small python script that uses a test function from the SALib package as the problem to analyse. import numpy as np from SALib.test_functions import Ishigami #First we load the data data_dir = \"data\" X_sobol = np . loadtxt ( f \" { data_dir } /x_sobol.csv\" ) X_morris = np . loadtxt ( f \" { data_dir } /x_morris.csv\" ) X_lhs = np . loadtxt ( f \" { data_dir } /x_lhs.csv\" ) #generate the y values y_sobol = Ishigami . evaluate ( X_sobol ) y_morris = Ishigami . evaluate ( X_morris ) y_lhs = Ishigami . evaluate ( X_lhs ) #store the results in files np . savetxt ( f \" { data_dir } /y_lhs.csv\" , y_lhs ) np . savetxt ( f \" { data_dir } /y_morris.csv\" , y_morris ) np . savetxt ( f \" { data_dir } /y_sobol.csv\" , y_sobol ) The next and final step is to analyse the just evaluated design of experiments using the SA methods and generate the report. Docker Python Executable docker run --rm -v ` pwd ` /output:/output \\ -v ` pwd ` /data:/data \\ emeraldit/gsareport -p /data/problem.json -d /data -o /output python GSAreport.py -p problem.json -d data_dir -o output_dir ./GSAreport -p problem.json -d data_dir -o output_dir Here we give an additional volume to our docker image such that we can access the generated output report in the output directory. We ommit the --sample instruction here such that it will load the data and start the analysis.","title":"Quick start"},{"location":"usecases/#common-use-cases","text":"There are three main steps in using the GSA report application, first to generate designs of experiments (the input files), second to evaluate these design of experiments and store them as y_*.csv files (using your own logic / simulators / real world experiments), and last but not least to load the data and perform the sensitivity analysis. To generate the samples for evaluation by your own code / simulator you can run the following docker command: Docker Python Executable docker run --rm \\ -v ` pwd ` /data:/data \\ emeraldit/gsareport -p /data/problem.json -d /data --sample --samplesize 1000 Here we run a docker image called emeraldit/gsareport , which is the GSAreport program packaged with all the required dependencies. The following line that start with -v creates a volume, sharing the folder data in our current working directory with the docker image (in location /data on the image). That way the program can access the data directory to store the design of experiment files ( x_*.csv ). python GSAreport.py -p problem.json -d ` pwd ` /data --sample --samplesize 1000 ./GSAreport -p problem.json -d ` pwd ` /data --sample --samplesize 1000 We give the following parameters to the program, -p to specify where to find the problem.json file (in the shared volume), -d to specify where to find the data, --sample to tell the program to generate the samples and --samplesize 1000 to specify that we want designs of experiments with 1000 samples. After running this step we will have 3 .csv files in our pwd /data folder (X_sobol.csv, X_lhs.csv and X_morris.csv). Using these 3 files we can generate the corresponding output files (outside of this software) using any tool. In this example we run a small python script that uses a test function from the SALib package as the problem to analyse. import numpy as np from SALib.test_functions import Ishigami #First we load the data data_dir = \"data\" X_sobol = np . loadtxt ( f \" { data_dir } /x_sobol.csv\" ) X_morris = np . loadtxt ( f \" { data_dir } /x_morris.csv\" ) X_lhs = np . loadtxt ( f \" { data_dir } /x_lhs.csv\" ) #generate the y values y_sobol = Ishigami . evaluate ( X_sobol ) y_morris = Ishigami . evaluate ( X_morris ) y_lhs = Ishigami . evaluate ( X_lhs ) #store the results in files np . savetxt ( f \" { data_dir } /y_lhs.csv\" , y_lhs ) np . savetxt ( f \" { data_dir } /y_morris.csv\" , y_morris ) np . savetxt ( f \" { data_dir } /y_sobol.csv\" , y_sobol ) The next and final step is to analyse the just evaluated design of experiments using the SA methods and generate the report. Docker Python Executable docker run --rm -v ` pwd ` /output:/output \\ -v ` pwd ` /data:/data \\ emeraldit/gsareport -p /data/problem.json -d /data -o /output python GSAreport.py -p problem.json -d data_dir -o output_dir ./GSAreport -p problem.json -d data_dir -o output_dir Here we give an additional volume to our docker image such that we can access the generated output report in the output directory. We ommit the --sample instruction here such that it will load the data and start the analysis.","title":"Common use cases"},{"location":"gsareport/gsareport/","text":"SAReport source SAReport ( problem , top = 20 , name = 'SAexperiment' , output_dir = 'output/' , data_dir = 'data/' , model_samples = 1000 , seed = 42 ) SAReport object to generate samples, load samples and generate the sensitivity analysis report. Args problem (dict) : The problem definition. top (int) : The number of important features we are interested in. name (str) : The name of the experiment. output_dir (str) : The location where the report will be written to. data_dir (str) : The directory path where the data can be loaded from. It expects the following combinations of csv files: either x.csv and y.csv or, x_lhs.csvm y_lhs.csv and x_morris.csv, y_morris.csv and x_sobol.csv, y_sobol.csv. At least one of these file pairs should be present. model_samples (int) : The number of samples to generated using the Random Forest model. seed (int) : random seed. Methods: .generateSamples source . generateSamples ( sample_size = 10000 ) Generate samples for the different SA techniques. Args sample_size (int) : The number of samples to generate for each design of experiments. Returns list : A list containing the 3 design of experiments (3 times sample_size samples). Example Generate 500 samples per DOE. $ report = SAReport(problem, \"Test problem\") $ lhs,morris,sobol = report.generateSamples(500) .storeSamples source . storeSamples () Store the generated samples to csv files in the data dir. Example Generate 500 samples per DOE and store them in the data directory. $ report = SAReport(problem, \"Test problem\") $ report.generateSamples(500) $ report.storeSamples(\"data\") .trainModel source . trainModel ( X , y ) Train a Random Forest regressor on provided data to generate different samples. Args X (array) : Two dimensional array of instances and features. y (list) : List with target values for X. Returns float : The cross validation score of the RF model. Example Train a model for a given input and output. $ report = SAReport(problem, \"Test problem\") $ r2 = report.trainModel(X,y) .sampleUsingModel source . sampleUsingModel () Use the trained model to generate the samples for each SA method. .loadData source . loadData () Loads the X and y data csv files generated by the sampling and by an external evaluation function. Example Load the csv files from the data folder. $ report.loadData() .analyse source . analyse () Perform the SA analysis steps and generate the report.","title":"Class reference"},{"location":"gsareport/gsareport/#_1","text":"","title":""},{"location":"gsareport/gsareport/#sareport","text":"source SAReport ( problem , top = 20 , name = 'SAexperiment' , output_dir = 'output/' , data_dir = 'data/' , model_samples = 1000 , seed = 42 ) SAReport object to generate samples, load samples and generate the sensitivity analysis report. Args problem (dict) : The problem definition. top (int) : The number of important features we are interested in. name (str) : The name of the experiment. output_dir (str) : The location where the report will be written to. data_dir (str) : The directory path where the data can be loaded from. It expects the following combinations of csv files: either x.csv and y.csv or, x_lhs.csvm y_lhs.csv and x_morris.csv, y_morris.csv and x_sobol.csv, y_sobol.csv. At least one of these file pairs should be present. model_samples (int) : The number of samples to generated using the Random Forest model. seed (int) : random seed. Methods:","title":"SAReport"},{"location":"gsareport/gsareport/#generatesamples","text":"source . generateSamples ( sample_size = 10000 ) Generate samples for the different SA techniques. Args sample_size (int) : The number of samples to generate for each design of experiments. Returns list : A list containing the 3 design of experiments (3 times sample_size samples). Example Generate 500 samples per DOE. $ report = SAReport(problem, \"Test problem\") $ lhs,morris,sobol = report.generateSamples(500)","title":".generateSamples"},{"location":"gsareport/gsareport/#storesamples","text":"source . storeSamples () Store the generated samples to csv files in the data dir. Example Generate 500 samples per DOE and store them in the data directory. $ report = SAReport(problem, \"Test problem\") $ report.generateSamples(500) $ report.storeSamples(\"data\")","title":".storeSamples"},{"location":"gsareport/gsareport/#trainmodel","text":"source . trainModel ( X , y ) Train a Random Forest regressor on provided data to generate different samples. Args X (array) : Two dimensional array of instances and features. y (list) : List with target values for X. Returns float : The cross validation score of the RF model. Example Train a model for a given input and output. $ report = SAReport(problem, \"Test problem\") $ r2 = report.trainModel(X,y)","title":".trainModel"},{"location":"gsareport/gsareport/#sampleusingmodel","text":"source . sampleUsingModel () Use the trained model to generate the samples for each SA method.","title":".sampleUsingModel"},{"location":"gsareport/gsareport/#loaddata","text":"source . loadData () Loads the X and y data csv files generated by the sampling and by an external evaluation function. Example Load the csv files from the data folder. $ report.loadData()","title":".loadData"},{"location":"gsareport/gsareport/#analyse","text":"source . analyse () Perform the SA analysis steps and generate the report.","title":".analyse"}]}